"""
AGENTE 1 â€” SCOUT
Monitora tendÃªncias virais em Marketing Digital, Meta Ads, Google Ads, IA e NegÃ³cios.
Roda 3x por semana (seg/qua/sex) e retorna top 5 trends com score de viralidade.
"""

import os
import json
import asyncio
import httpx
from datetime import datetime
from anthropic import Anthropic

# â”€â”€â”€ CONFIGURAÃ‡Ã•ES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
NOTION_TOKEN      = os.getenv("NOTION_TOKEN")
NOTION_DB_TRENDS  = os.getenv("NOTION_DB_TRENDS")   # ID da base de trends no Notion

client = Anthropic(api_key=ANTHROPIC_API_KEY)

# â”€â”€â”€ FONTES MONITORADAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCES = [
    # RSS / feeds pÃºblicos
    "https://feeds.feedburner.com/socialmediaexaminer",
    "https://blog.hootsuite.com/feed/",
    "https://searchengineland.com/feed",
    "https://www.jonloomer.com/feed/",
    "https://marketingland.com/feed",
    # Reddit via API pÃºblica (sem auth)
    "https://www.reddit.com/r/PPC/top.json?limit=10&t=week",
    "https://www.reddit.com/r/FacebookAds/top.json?limit=10&t=week",
    "https://www.reddit.com/r/artificial/top.json?limit=10&t=week",
    "https://www.reddit.com/r/marketing/top.json?limit=10&t=week",
    "https://www.reddit.com/r/Entrepreneur/top.json?limit=10&t=week",
]

TOPICS = [
    "Meta Ads", "Google Ads", "Marketing Digital",
    "InteligÃªncia Artificial", "NegÃ³cios", "TrÃ¡fego Pago",
    "Instagram", "TikTok Ads", "IA Generativa", "AutomaÃ§Ã£o"
]


# â”€â”€â”€ FUNÃ‡Ã•ES DE BUSCA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_reddit(url: str, client_http: httpx.AsyncClient) -> list[dict]:
    """Busca posts do Reddit via JSON pÃºblico."""
    try:
        headers = {"User-Agent": "wavy-scout-bot/1.0"}
        resp = await client_http.get(url, headers=headers, timeout=10)
        data = resp.json()
        posts = []
        for post in data.get("data", {}).get("children", []):
            p = post["data"]
            posts.append({
                "title": p.get("title", ""),
                "url": f"https://reddit.com{p.get('permalink', '')}",
                "score": p.get("score", 0),
                "comments": p.get("num_comments", 0),
                "source": "Reddit"
            })
        return posts
    except Exception as e:
        print(f"Erro Reddit {url}: {e}")
        return []


async def fetch_rss(url: str, client_http: httpx.AsyncClient) -> list[dict]:
    """Busca artigos via RSS."""
    try:
        resp = await client_http.get(url, timeout=10, follow_redirects=True)
        import xml.etree.ElementTree as ET
        root = ET.fromstring(resp.text)
        items = []
        for item in root.iter("item"):
            title = item.findtext("title", "")
            link  = item.findtext("link", "")
            if title and link:
                items.append({
                    "title": title.strip(),
                    "url": link.strip(),
                    "score": 0,
                    "comments": 0,
                    "source": url.split("/")[2]
                })
        return items[:5]
    except Exception as e:
        print(f"Erro RSS {url}: {e}")
        return []


async def collect_all_content() -> list[dict]:
    """Coleta conteÃºdo de todas as fontes em paralelo."""
    all_content = []
    async with httpx.AsyncClient() as http:
        tasks = []
        for source in SOURCES:
            if "reddit.com" in source:
                tasks.append(fetch_reddit(source, http))
            else:
                tasks.append(fetch_rss(source, http))
        
        results = await asyncio.gather(*tasks)
        for result in results:
            all_content.extend(result)
    
    return all_content


# â”€â”€â”€ ANÃLISE COM CLAUDE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_trends_with_claude(raw_content: list[dict]) -> list[dict]:
    """
    Usa Claude para analisar o conteÃºdo coletado e retornar
    as top 5 trends mais relevantes com score de viralidade.
    """
    # Prepara resumo do conteÃºdo pra enviar pro Claude
    content_summary = "\n".join([
        f"- [{item['source']}] {item['title']} (score: {item.get('score', 0)}, comments: {item.get('comments', 0)})"
        for item in raw_content[:80]  # limita pra nÃ£o explodir o contexto
    ])

    today = datetime.now().strftime("%d/%m/%Y")

    response = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=2000,
        messages=[{
            "role": "user",
            "content": f"""VocÃª Ã© um analista de tendÃªncias de marketing digital especializado em conteÃºdo viral para Instagram.

Data de hoje: {today}

Abaixo estÃ£o os tÃ­tulos e fontes coletados de blogs, Reddit e feeds de marketing digital nas Ãºltimas horas:

{content_summary}

Sua tarefa:
1. Identifique as TOP 5 tendÃªncias mais relevantes e virais nos temas: {', '.join(TOPICS)}
2. Para cada trend, atribua um score de viralidade de 0 a 100 baseado em:
   - RelevÃ¢ncia para criadores de conteÃºdo de marketing/negÃ³cios no Brasil
   - Potencial de engajamento no Instagram
   - Novidade e timing (algo que estÃ¡ explodindo agora)
   - Aplicabilidade para pequenos e mÃ©dios empreendedores

Retorne APENAS um JSON vÃ¡lido neste formato (sem explicaÃ§Ãµes, sem markdown):
{{
  "trends": [
    {{
      "titulo": "TÃ­tulo claro e descritivo da trend em portuguÃªs",
      "descricao": "2-3 frases explicando o que estÃ¡ acontecendo e por que Ã© relevante agora",
      "topico": "Meta Ads | Google Ads | Marketing Digital | IA | NegÃ³cios",
      "score_viralidade": 85,
      "angulos_sugeridos": [
        "Ã‚ngulo 1 para carrossel ou reels",
        "Ã‚ngulo 2 para carrossel ou reels", 
        "Ã‚ngulo 3 para carrossel ou reels"
      ],
      "fonte": "Nome da fonte original"
    }}
  ],
  "data_coleta": "{today}",
  "total_fontes_analisadas": {len(raw_content)}
}}"""
        }]
    )

    try:
        text = response.content[0].text.strip()
        # Remove possÃ­vel markdown se vier
        if text.startswith("```"):
            text = text.split("```")[1]
            if text.startswith("json"):
                text = text[4:]
        return json.loads(text)
    except json.JSONDecodeError as e:
        print(f"Erro ao parsear JSON do Claude: {e}")
        print(f"Resposta: {response.content[0].text}")
        return {"trends": [], "data_coleta": today, "total_fontes_analisadas": 0}


# â”€â”€â”€ SALVAR NO NOTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def save_to_notion(trends_data: dict):
    """Salva as trends encontradas na base do Notion."""
    if not NOTION_TOKEN or not NOTION_DB_TRENDS:
        print("âš ï¸  Notion nÃ£o configurado â€” pulando salvamento.")
        return

    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    async with httpx.AsyncClient() as http:
        for trend in trends_data.get("trends", []):
            payload = {
                "parent": {"database_id": NOTION_DB_TRENDS},
                "properties": {
                    "TÃ­tulo": {
                        "title": [{"text": {"content": trend["titulo"]}}]
                    },
                    "DescriÃ§Ã£o": {
                        "rich_text": [{"text": {"content": trend["descricao"]}}]
                    },
                    "TÃ³pico": {
                        "select": {"name": trend["topico"]}
                    },
                    "Score Viralidade": {
                        "number": trend["score_viralidade"]
                    },
                    "Fonte": {
                        "rich_text": [{"text": {"content": trend["fonte"]}}]
                    },
                    "Status": {
                        "select": {"name": "Nova"}
                    },
                    "Data Coleta": {
                        "date": {"start": datetime.now().strftime("%Y-%m-%d")}
                    }
                }
            }
            try:
                resp = await http.post(
                    "https://api.notion.com/v1/pages",
                    headers=headers,
                    json=payload
                )
                if resp.status_code == 200:
                    print(f"âœ… Trend salva no Notion: {trend['titulo']}")
                else:
                    print(f"âŒ Erro Notion: {resp.status_code} â€” {resp.text}")
            except Exception as e:
                print(f"Erro ao salvar no Notion: {e}")


# â”€â”€â”€ EXECUTAR SCOUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def run_scout() -> dict:
    """FunÃ§Ã£o principal do Agente 1."""
    print("ğŸ” Agente 1 â€” Scout iniciado...")
    print(f"ğŸ“… Data: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
    print("ğŸ“¡ Coletando conteÃºdo das fontes...")

    # 1. Coleta conteÃºdo
    raw_content = await collect_all_content()
    print(f"âœ… {len(raw_content)} itens coletados de {len(SOURCES)} fontes")

    # 2. Analisa com Claude
    print("ğŸ§  Analisando tendÃªncias com Claude...")
    trends_data = analyze_trends_with_claude(raw_content)
    print(f"âœ… {len(trends_data.get('trends', []))} trends identificadas")

    # 3. Salva no Notion
    print("ğŸ“ Salvando no Notion...")
    await save_to_notion(trends_data)

    # 4. Exibe resultado
    print("\n" + "="*50)
    print("ğŸ“Š TOP 5 TRENDS DA SEMANA:")
    print("="*50)
    for i, trend in enumerate(trends_data.get("trends", []), 1):
        print(f"\n{i}. {trend['titulo']}")
        print(f"   ğŸ“Œ TÃ³pico: {trend['topico']}")
        print(f"   ğŸ”¥ Score: {trend['score_viralidade']}/100")
        print(f"   ğŸ“ {trend['descricao']}")

    return trends_data


if __name__ == "__main__":
    result = asyncio.run(run_scout())
    # Salva resultado em arquivo pra passar pro Agente 2
    with open("/tmp/trends_result.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print("\nâœ… Scout finalizado. Resultado salvo em /tmp/trends_result.json")
